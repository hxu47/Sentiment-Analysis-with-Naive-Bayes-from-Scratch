{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Reviews Sentiment Analysis with Naive Bayes\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "In this project, I will build a multinomial naive bayes classifier to predict whether a movie review is positive or negative.  As part of the project, I will implement *k*-fold cross validation testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes classifiers for text documents\n",
    "\n",
    "A text classifier predicts to which class, $c$, an unknown document $d$ belongs. In our case, the predictions are binary: $c=0$ for negative movie review and $c=1$ for positive movie review. We can think about classification mathematically as picking the most likely class:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~P(c|d)\n",
    "$$\n",
    "\n",
    "We can replace $P(c|d)$, using Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(c | d) = \\frac{P(c)P(d|c)}{P(d)}\n",
    "$$\n",
    "\n",
    "to get the formula \n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~\\frac{P(c)P(d|c)}{P(d)}\n",
    "$$\n",
    "\n",
    "Since $P(d)$ is a constant for any given document $d$, we can use the following equivalent but simpler formula:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c)P(d|c)\n",
    "$$\n",
    "\n",
    "Training then consists of estimating $P(c)$ and $P(c|d)$, which will get to shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing documents\n",
    "\n",
    "Text classification requires a representation for document $d$. When loading a document, we first load the text and then tokenize the words, stripping away punctuation and stop words like *the*. The list of words is a fine representation for a document except that each document has a different length, which makes training most models problematic as they assume tabular data with a fixed number of features.  \n",
    "\n",
    "The simplest and most common approach is to: \n",
    "1. Create an overall vocabulary, $V$, created as a set of unique words across all documents in all classes. \n",
    "2. Sort the unique words in the vocab alphabetically so we standardize which word is associated with which word vector index. Then, the training features are those words.\n",
    "3. Then, one way to represent a document is with a binary word vector, with a 1 in each column represents __if that word is present in the document__. Something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>food</th>\n",
       "      <th>hong</th>\n",
       "      <th>kong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  food  hong  kong\n",
       "0    1     1     0     0\n",
       "1    0     0     1     1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=[[1,1,0,0],\n",
    "                        [0,0,1,1]], columns=['cat','food','hong','kong'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tends to work well for very short strings/documents, such as article titles or tweets. For longer documents, using a binary presence or absence loses information. Instead, it's better to __count the number of times each word is present__. For example, here are 3 documents and resulting word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>food</th>\n",
       "      <th>hong</th>\n",
       "      <th>kong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  food  hong  kong\n",
       "0    3     1     0     0\n",
       "1    0     0     2     2\n",
       "2    1     0     1     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = \"cats food cats cats\"\n",
    "d2 = \"hong kong hong kong\"\n",
    "d3 = \"cats in hong kong\"  # assume we strip stop words like \"in\"\n",
    "df = pd.DataFrame(data=[[3,1,0,0],\n",
    "                        [0,0,2,2],\n",
    "                        [1,0,1,1]],\n",
    "                  columns=['cat','food','hong','kong'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word vectors with fixed lengths are how most models expect data, including sklearn's implementation. (It's assuming Gaussian distributions for probability estimates where as we are assuming multinomial, but we can still shove our data in.)  Here's how to train a Naive Bayes model with sklearn using the trivial/toy `df` data and get the training set error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct = 3 / 3 = 100.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "X = df.values\n",
    "y = [0, 1, 1] # assume document classes\n",
    "sknb = GaussianNB()\n",
    "sknb.fit(X, y)\n",
    "y_pred = sknb.predict(X)\n",
    "print(f\"Correct = {np.sum(y==y_pred)} / {len(y)} = {100*np.sum(y==y_pred) / len(y):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is convenient to keep word vectors in a 2D matrix and it is what sklearn likes, I will use the same representation in this project. \n",
    "\n",
    "Functions used are:\n",
    "- Given the directory name, the function `load_docs()` will return a list of word lists where each word list is the raw list of tokens, typically with repeated words. \n",
    "- Then,  function `vocab()` will create the combined vocabulary as a mapping from word to  word feature index, starting with index 1.  Index 0 is reserved for unknown words.  Vocabulary $V$ should be a `defaultintdict()`, so that unknown words get mapped to value/index 0. \n",
    "- Finally, function `vectorize()` will convert that to a 2D matrix, one row per document:\n",
    "\n",
    "```\n",
    "neg = load_docs(neg_dir)\n",
    "pos = load_docs(pos_dir)\n",
    "V = vocab(neg,pos)\n",
    "vneg = vectorize_docs(neg, V)\n",
    "vpos = vectorize_docs(pos, V)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `defaultintdict` class behaves exactly like defaultdict(int) except d['foo'] does **not** add 'foo' to dictionary d. (Booo for that default behavior in defaultdict!)\n",
    "\n",
    "```\n",
    "class defaultintdict(dict):\n",
    "    def __init__(self): # Create dictionary of ints\n",
    "        self._factory=int\n",
    "        super().__init__()\n",
    "\n",
    "    def __missing__(self, key): \"Override default behavior so missing returns 0\"\n",
    "        return 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to estimate $P(c)$ and $P(d|c)$ for all classes and documents. Estimating $P(c)$ is easy: it's just the number of documents in class $c$ divided by the total number of documents. To estimate $P(d | c)$, Naive Bayes assumes that each word is *conditionally independent*, given the class, meaning:\n",
    "\n",
    "$$\n",
    "P(d | c) = \\prod_{w \\in d} P(w | c)\n",
    "$$\n",
    "\n",
    "so that gives us:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c) \\prod_{w \\in d} P(w | c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $w$ is not the unique words in $d$, so the product includes $P(w|c)$ 5 times if $w$ appears 5 times in $d$.\n",
    "\n",
    "Because we are going to use word counts, not binary word vectors, in fixed-length vectors, we need to include $P(w|c)$ explicitly multiple times for repeated $w$ in $d$:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c) \\prod_{w \\in V} P(w | c)^{n_w(d)}\n",
    "$$\n",
    "\n",
    "where $n_w(d)$ is the number of times $w$ occurs in $d$, $V$ is the overall vocabulary (set of unique words from all documents); $n_w(d)=0$ when $w$ isn't present in $d$.\n",
    "\n",
    "Now we have to figure out how to estimate $P(w | c)$, the probability of seeing word $w$ given that we're looking at a document from class $c$. That's just the number of times $w$ appears in all documents from class $c$ divided by the total number of words (including repeats) in all documents from class $c$:\n",
    "\n",
    "$$P(w | c) = \\frac{wordcount(w,c)}{wordcount(c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "Once we have the appropriate parameter estimates, we have a model that can make predictions in an ideal setting:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c) \\prod_{w \\in V} P(w | c)^{n_w(d)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding $P(w|c)=0$\n",
    "\n",
    "If word $w$ does not exist in class $c$ (but is in $V$), then overall product goes to 0 (and when we take the log below, the classifier would try to evaluate $log(0)$, which is undefined).  To solve the problem, we use *Laplace Smoothing*, which just means adding 1 to each word count when computing $P(w|c)$ and making sure to compensate by adding $|V|$ to the denominator (adding 1 for each vocabulary word):\n",
    "\n",
    "$$P(w | c) = \\frac{wordcount(w,c) + 1}{wordcount(c) + |V|}$$\n",
    "\n",
    "where $|V|$ is the size of the vocabulary for all documents in all classes.  Adding this to the denominator, keeps  $P(w|c)$ a probability. This way, even if $wordcount(w,c)$ is 0, this ratio > 0.  (Note: Each doc's word vector has length $|V|$. During training, any $w$ not found in docs of $c$, will have word count 0. Summing these gets us just total number of words in $c$. However, when we add +1, then $c$ looks like it has every word in $V$.  Hence, we must divide by $|V|$ not $|V_c|$. If $w$ is not in any doc of class $c$ then $P(w|c)=1/(wordcount(c)+|V|)$, which is a very low probability.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Dealing with missing words\n",
    "\n",
    "Laplace smoothing deals with $w$ that are in the vocabulary $V$ but that are not in a class, hence, giving $wordcount(w,c)=0$ for some $c$. There's one last slightly different problem. If a future unknown document contains a word not in $V$ (i.e., not in the training data), then what should $wordcount(w,c)$ be?  Probably not 0 because that would mean we had data indicating it does not appear in class $c$ when we have *no* training data on it.\n",
    "\n",
    "To be strictly correct and keep $P(w | c)$ a probability in the presence of unknown words, all we have to do is add 1 to the denominator in addition to the Laplace smoothing changes:\n",
    "\n",
    "$$P(w | c) = \\frac{wordcount(w,c) + 1}{wordcount(c) + |V| + 1}$$\n",
    "\n",
    "We are lumping all unknown words into a single \"wildcard\" word that exists in every $V$. That has the effect of increasing the overall vocabulary size for class $c$ to include room for an unknown word (and all unknown words map to that same spot). In this way, an unknown word gets probability:\n",
    "\n",
    "$$P(unknown|c) = \\frac{0 + 1}{wordcount(c) + |V| + 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, this is no big deal as all classes will get the same nudge for the unknown word so classification won't be affected.\n",
    "\n",
    "To deal with unknown words in the project, we can reserve word index 0 to mean unknown word. All words in the training vocabulary start at index 1. So, if we normally have $|V|$ words in the training vocabulary, we will now have $|V|+1$; no word will ever have 0 word count. Each word vector will be of length $|V|+1$.  \n",
    "\n",
    "The `vocab()` function in your project returns $|V| = |uniquewords|+1$ to handle the unknown word wildcard.  Once computed, the size of the vocabulary should never change; all word vectors are size $|V|$.\n",
    "\n",
    "With this new adjusted estimate of $P(w|c)$, we can simplify the overall prediction problem to use $w \\in V$:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c) \\prod_{w \\in V} P(w | c)^{n_w(d)}\n",
    "$$\n",
    "\n",
    "That means we can use dot products for prediction, which is faster than iterating in python through unique document words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Floating point underflow\n",
    "\n",
    "The first problem involves the limitations of floating-point arithmetic in computers. Because the probabilities are less than one and there could be tens of thousands multiplied together, we risk floating-point underflow. That just means that eventually the product will attenuate to zero and our classifier is useless.  The solution is simply to take the log of the right hand side because log is monotonic and won't affect the $argmax$:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} \\left \\{ log(P(c)) + \\sum_{w \\in V} log(P(w | c)^{n_w(d)}) \\right \\}\n",
    "$$\n",
    "\n",
    "Or,\n",
    "\n",
    "$$\n",
    "c^* = \\underset{c}{argmax} \\left \\{ log(P(c)) + \\sum_{w \\in V} n_w(d) \\times log(P(w | c)) \\right \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed issues\n",
    "\n",
    "For large data sets, Python loops often are too slow and so we have to rely on vector operations, which are implemented in C. For example, the `predict(X)` method receives a 2D matrix of word vectors and must make a prediction for each one. The temptation is to write the very readable:\n",
    "\n",
    "```\n",
    "y_pred = []\n",
    "for each row d in X:\n",
    "    y_pred = prediction for d\n",
    "return y_pred\n",
    "```\n",
    "\n",
    "But, we can use the built-in `numpy` functions such as `np.dot` (same as the `@` operator) and apply functions across vectors. For example, if I have a vector, $v$, and I'd like the log of each value, don't write a loop. Use `np.log(v)`, which will give us a vector with the results.\n",
    "\n",
    "My `predict()` method consists primarily of a matrix-vector multiplication per class followed by `argmax`. My implementation is, oddly, twice as fast as sklearn and appears to be more accurate for 4-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "To submit your project, ensure that your `bayes.py` file is submitted to your repository. That file must be in the root of your `bayes`-*userid* repository.  It should not have a main program; it should consist of a collection of functions. You must implement the following functions:\n",
    "\n",
    "* `load_docs(docs_dirname)`\n",
    "* `vocab(neg, pos)`\n",
    "* `vectorize(V, docwords)`\n",
    "* `vectorize_docs(docs, V)`\n",
    "* `kfold_CV(model, X, y, k=4)` (You must implement manually; don't use sklearn's version)\n",
    "\n",
    "and implement class `NaiveBayes621` with these methods\n",
    "\n",
    "* `fit(self, X, y)`\n",
    "* `predict(self, X)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "Gather a labeled dataset containing text samples with corresponding sentiment labels (e.g., positive, negative, neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from bayes import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class defaultintdict(dict):\n",
    "    \"\"\"\n",
    "    Behaves exactly like defaultdict(int) except d['foo'] does NOT\n",
    "    add 'foo' to dictionary d. (Booo for that default behavior in\n",
    "    defaultdict!)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._factory=int\n",
    "        super().__init__()\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        return 0\n",
    "\n",
    "\n",
    "def filelist(root) -> Sequence[str]:\n",
    "    \"\"\"Return a fully-qualified list of filenames under root directory; sort names alphabetically.\"\"\"\n",
    "    allfiles = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            allfiles.append(os.path.join(path, name))\n",
    "    return sorted(allfiles)\n",
    "\n",
    "\n",
    "def get_text(filename:str) -> str:\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s\n",
    "\n",
    "def words(text:str) -> Sequence[str]:\n",
    "    \"\"\"\n",
    "    Given a string, return a list of words normalized as follows.\n",
    "    Split the string to make words first by using regex compile() function\n",
    "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n",
    "    char with a space character.\n",
    "    Split on space to get word list.\n",
    "    Ignore words < 3 char long.\n",
    "    Lowercase all words\n",
    "    Remove English stop words\n",
    "    \"\"\"\n",
    "    ctrl_chars = '\\x00-\\x1f'\n",
    "    regex = re.compile(r'[' + ctrl_chars + string.punctuation + '0-9\\r\\t\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if len(w) > 2]  # ignore a, an, to, at, be, ...\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "\n",
    "def load_docs(docs_dirname:str) -> Sequence[Sequence]:\n",
    "    \"\"\"\n",
    "    Load all .txt files under docs_dirname and return a list of word lists, one per doc.\n",
    "    Ignore empty and non \".txt\" files.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    filenames_list = filelist(docs_dirname)\n",
    "    for filename in filenames_list:\n",
    "        text = get_text(filename)\n",
    "        words_list = words(text)\n",
    "        docs.append(words_list)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quest', 'camelot', 'warner', 'bros', 'feature', 'length', 'fully', 'animated', 'attempt', 'steal', 'clout', 'disney', 'cartoon', 'empire', 'mouse', 'reason', 'worried', 'recent', 'challenger', 'throne', 'fall', 'promising', 'flawed', 'century', 'fox', 'production', 'anastasia', 'disney', 'hercules', 'lively', 'cast', 'colorful', 'palate', 'beat', 'hands', 'came', 'time', 'crown', 'best', 'piece', 'animation', 'year', 'contest', 'quest', 'camelot', 'pretty', 'dead', 'arrival', 'magic', 'kingdom', 'mediocre', 'pocahontas', 'keeping', 'score', 'isn', 'nearly', 'dull', 'story', 'revolves', 'adventures', 'free', 'spirited', 'kayley', 'voiced', 'jessalyn', 'gilsig', 'early', 'teen', 'daughter', 'belated', 'knight', 'king', 'arthur', 'round', 'table', 'kayley', 'dream', 'follow', 'father', 'footsteps', 'gets', 'chance', 'evil', 'warlord', 'ruber', 'gary', 'oldman', 'round', 'table', 'member', 'gone', 'bad', 'steals', 'arthur', 'magical', 'sword', 'excalibur', 'accidentally', 'loses', 'dangerous', 'booby', 'trapped', 'forest', 'help', 'hunky', 'blind', 'timberland', 'dweller', 'garrett', 'carey', 'elwes', 'headed', 'dragon', 'eric', 'idle', 'don', 'rickles', 'arguing', 'kayley', 'just', 'able', 'break', 'medieval', 'sexist', 'mold', 'prove', 'worth', 'fighter', 'arthur', 'quest', 'camelot', 'missing', 'pure', 'showmanship', 'essential', 'element', 'expected', 'climb', 'high', 'ranks', 'disney', 'differentiates', 'quest', 'given', 'saturday', 'morning', 'cartoon', 'subpar', 'animation', 'instantly', 'forgettable', 'songs', 'poorly', 'integrated', 'computerized', 'footage', 'compare', 'kayley', 'garrett', 'run', 'angry', 'ogre', 'herc', 'battle', 'hydra', 'rest', 'case', 'characters', 'stink', 'remotely', 'interesting', 'film', 'race', 'bland', 'end', 'tie', 'win', 'dragon', 'comedy', 'shtick', 'awfully', 'cloying', 'shows', 'signs', 'pulse', 'fans', 'early', 'tgif', 'television', 'line', 'thrilled', 'jaleel', 'urkel', 'white', 'bronson', 'balki', 'pinchot', 'sharing', 'footage', 'scenes', 'nicely', 'realized', 'loss', 'recall', 'specific', 'actors', 'providing', 'voice', 'talent', 'enthusiastic', 'paired', 'singers', 'don', 'sound', 'thing', 'like', 'big', 'musical', 'moments', 'jane', 'seymour', 'celine', 'dion', 'strain', 'mess', 'good', 'aside', 'fact', 'children', 'probably', 'bored', 'watching', 'adults', 'quest', 'camelot', 'grievous', 'error', 'complete', 'lack', 'personality', 'personality', 'learn', 'mess', 'goes', 'long', 'way']\n"
     ]
    }
   ],
   "source": [
    "neg_dir = 'review_polarity/txt_sentoken/neg' \n",
    "pos_dir = 'review_polarity/txt_sentoken/pos'\n",
    "\n",
    "neg = load_docs(neg_dir) \n",
    "pos = load_docs(pos_dir)\n",
    "\n",
    "assert len(neg) == 1000\n",
    "assert len(pos) == 1000\n",
    "print(neg[3]) # print the list of word in a document that are labeled as 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a vocabulary from the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(neg:Sequence[Sequence], pos:Sequence[Sequence]) -> dict:\n",
    "    \"\"\"\n",
    "    Given neg and pos lists of word lists, construct a mapping from word to word index.\n",
    "    Use index 0 to mean unknown word, '__unknown__'. The real words start from index one.\n",
    "    The words should be sorted so the first vocabulary word is index one.\n",
    "    The length of the dictionary is |uniquewords|+1 because of \"unknown word\".\n",
    "    |V| is the length of the vocabulary including the unknown word slot.\n",
    "\n",
    "    Sort the unique words in the vocab alphabetically so we standardize which\n",
    "    word is associated with which word vector index.\n",
    "\n",
    "    E.g., given neg = [['hi']] and pos=[['mom']], return:\n",
    "\n",
    "    V = {'__unknown__':0, 'hi':1, 'mom:2}\n",
    "\n",
    "    and so |V| is 3\n",
    "    \"\"\"\n",
    "    V_neg = set(np.concatenate(neg))\n",
    "    V_pos = set(np.concatenate(pos))\n",
    "    allwords = sorted(V_neg.union(V_pos))   # a list of all words except 'unknown'\n",
    "    allwords.insert(0, '__unknown__') # insert 'unknown' at the beginning of the list\n",
    "\n",
    "    # convert the list to dictionary\n",
    "    V = defaultintdict()\n",
    "    for ind, word in enumerate(allwords):\n",
    "        V[word] = ind\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38373\n",
      "['__unknown__' 'aaa' 'aaaaaaaaah' ... 'zwigoff' 'zycie' 'zzzzzzz']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "V = vocab(neg,pos)\n",
    "print(len(V))\n",
    "\n",
    "allwords = np.array([*V.keys()])\n",
    "print(allwords)\n",
    "\n",
    "print(V['__unknown__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acceptable', 'accompanies', 'alek', 'allows', 'amistad', 'amnesia', 'anti', 'armored', 'arty', 'atrophied', 'authentically', 'barbecue', 'bastille', 'battles', 'beatrice', 'bedtimes', 'bolt', 'bombarded', 'braun', 'breathed', 'cavern', 'characers', 'charms', 'cimino', 'comely', 'compensating', 'contentious', 'delayed', 'deliveree', 'denise', 'dependant', 'deuce', 'disintegrated', 'doom', 'embarassed', 'enterprises', 'entrepreneur', 'eurocentrism', 'examinations', 'existing', 'exposure', 'fahdlan', 'fer', 'flirts', 'franken', 'gait', 'gloat', 'goal', 'groaning', 'groundbreaking', 'homeworld', 'hovertank', 'independance', 'inputs', 'instinctively', 'invincibility', 'kermit', 'lanai', 'lava', 'lavender', 'libidinous', 'locating', 'meshes', 'metamorphoses', 'moff', 'moribund', 'mortal', 'neptune', 'observatory', 'onstage', 'orbiting', 'overemotional', 'overly', 'paradise', 'paramedic', 'parent', 'paz', 'portion', 'prays', 'pseudonym', 'psycholically', 'quinland', 'redcoats', 'robo', 'sacred', 'shorten', 'silence', 'sincerely', 'solution', 'straits', 'supernaturally', 'taste', 'tryst', 'uneasiness', 'uninterrupted', 'walkway', 'wasting', 'won', 'xer', 'yield']\n"
     ]
    }
   ],
   "source": [
    "rs = np.random.RandomState(42) # get same list back each time\n",
    "idx = rs.randint(0,len(V),size=100)\n",
    "allwords = np.array([*V.keys()])\n",
    "subset = allwords[idx]\n",
    "print(sorted(subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Represent each text sample as a vector of word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(V:dict, docwords:Sequence) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return a row vector (based upon V) for docwords. The first element of the\n",
    "    returned vector is the count of unknown words. So |V| is |uniquewords|+1.\n",
    "    \"\"\"\n",
    "    row_vector = np.zeros(len(V))\n",
    "    for word in docwords:\n",
    "            word_index = V.get(word)  \n",
    "            if word_index:\n",
    "                row_vector[word_index] += 1\n",
    "            else:     # if the word is unknown\n",
    "                row_vector[0] += 1\n",
    "    return row_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_docs(docs:Sequence, V:dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return a matrix where each row represents a documents word vector.\n",
    "    Each column represents a single word feature. There are |V|+1\n",
    "    columns because we leave an extra one for the unknown word in position 0.\n",
    "    Invoke vector(V,docwords) to vectorize each doc for each row of matrix\n",
    "    :param docs: list of word lists, one per doc\n",
    "    :param V: Mapping from word to index; e.g., first word -> index 1\n",
    "    :return: numpy 2D matrix with word counts per doc: ndocs x nwords\n",
    "    \"\"\"\n",
    "    D = np.zeros((len(docs), len(V)))\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc:\n",
    "            word_index = V.get(word)  \n",
    "            if word_index:\n",
    "                D[i, word_index] += 1\n",
    "            else:     # if the word is unknown\n",
    "                D[i, 0] += 1\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vneg = vectorize_docs(neg, V)\n",
    "vpos = vectorize_docs(pos, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vneg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Construct the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the training data\n",
    "X = np.vstack([vneg, vpos])\n",
    "y = np.vstack([np.zeros(shape=(len(vneg), 1)),\n",
    "               np.ones(shape=(len(vpos), 1))]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test *vectorize* function\n",
    "d1 = vectorize(V, words(\"mostly very funny , the story is quite appealing.\"))\n",
    "d2 = vectorize(V, words(\"there is already a candidate for the worst of 1997.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = len(V)\n",
    "assert len(d1)==p, f\"d1 should be 1x{p} but is 1x{len(d1)}\"\n",
    "assert len(d2)==p, f\"d2 should be 1x{p} but is 1x{len(d2)}\"\n",
    "d1_idx = np.nonzero(d1)\n",
    "d2_idx = np.nonzero(d2)\n",
    "true_d1_idx = np.array([ 1367, 13337, 26872, 32570 ])\n",
    "true_d2_idx = np.array([ 4676, 37932 ])\n",
    "assert (d1_idx==true_d1_idx).all(), f\"{d1_idx} should be {true_d1_idx}\"\n",
    "assert (d2_idx==true_d2_idx).all(), f\"{d2_idx} should be {true_d2_idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implement and test Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes621:\n",
    "    \"\"\"\n",
    "    This object behaves like a sklearn model with fit(X,y) and predict(X) functions.\n",
    "    Limited to two classes, 0 and 1 in the y target.\n",
    "    \"\"\"\n",
    "    def fit(self, X:np.ndarray, y:np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Given 2D word vector matrix X, one row per document, and 1D binary vector y\n",
    "        train a Naive Bayes classifier assuming a multinomial distribution for\n",
    "        p(w,c), the probability of word exists in class c. p(w,c) is estimated by\n",
    "        the number of times w occurs in all documents of class c divided by the\n",
    "        total words in class c. p(c) is estimated by the number of documents\n",
    "        in c divided by the total number of documents.\n",
    "\n",
    "        The first column of X is a column of zeros to represent missing vocab words.\n",
    "        \"\"\"\n",
    "        self.V_length = X.shape[1]  # |V|, the size of the vocabulary for all documents in all classes\n",
    "\n",
    "        # P(c)\n",
    "        self.P_0 = (y == 0).sum() / len(y)  # P(0)\n",
    "        self.P_1 = (y == 1).sum() / len(y)  # P(1)\n",
    "\n",
    "        # P(w|c)  = (wordcount(w, c) + 1) / (wordcount(c) + |V| + 1)\n",
    "        self.P_w_0 = (X[y == 0].sum(axis=0) + 1) / (X[y == 0].sum() + self.V_length + 1)\n",
    "        self.P_w_1 = (X[y == 1].sum(axis=0) + 1) / (X[y == 1].sum() + self.V_length + 1)\n",
    "        \n",
    "    def predict(self, X:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given 2D word vector matrix X, one row per document, return binary vector\n",
    "        indicating class 0 or 1 for each row of X.\n",
    "        \"\"\"\n",
    "        likelihoods_0 = np.log(self.P_0) + (X*np.log(self.P_w_0)).sum(axis=1)\n",
    "        likelihoods_1 = np.log(self.P_1) + (X*np.log(self.P_w_1)).sum(axis=1)\n",
    "        return 1 - (likelihoods_0 > likelihoods_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9725\n",
      "Correct = 1945 / 2000 = 97.2%\n"
     ]
    }
   ],
   "source": [
    "model = NaiveBayes621()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "accuracy = np.sum(y==y_pred) / len(y)\n",
    "# print(f\"training accuracy {accuracy}\")\n",
    "print(accuracy)\n",
    "print(f\"Correct = {np.sum(y==y_pred)} / {len(y)} = {100*accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Implement and test k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_CV(model, X:np.ndarray, y:np.ndarray, k=4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run k-fold cross validation using model and 2D word vector matrix X and binary\n",
    "    y class vector. Return a 1D numpy vector of length k with the accuracies, the\n",
    "    ratios of correctly-identified documents to the total number of documents. You\n",
    "    can use KFold from sklearn to get the splits but must loop through the splits\n",
    "    with a loop to implement the cross-fold testing.  Pass random_state=999 to KFold\n",
    "    so we always get same sequence (wrong in practice) so student eval unit tests\n",
    "    are consistent. Shuffle the elements before walking the folds.\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=999)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = np.sum(y_test==y_pred) / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "    return np.array(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold [0.798 0.78  0.812 0.808] vs true [0.798 0.78  0.812 0.808]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Test my kfold_CV, so just use sklearn model\n",
    "sklearn_accuracies = kfold_CV(MultinomialNB(), X, y, k=4)\n",
    "true_sklearn_accuracies = np.array([0.798, 0.78, 0.812, 0.808])\n",
    "\n",
    "sklearn_avg = np.mean(sklearn_accuracies)\n",
    "true_avg = np.mean(true_sklearn_accuracies)\n",
    "\n",
    "print(f\"kfold {sklearn_accuracies} vs true {true_sklearn_accuracies}\")\n",
    "print(np.abs(sklearn_avg-true_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn kfold [0.798 0.78  0.812 0.808] vs my [0.798 0.78  0.812 0.808]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Compare NaiveBayes621 vs sklearn MultinomialNB() using my kfold_CV\n",
    "accuracies = kfold_CV(NaiveBayes621(), X, y, k=4)\n",
    "sklearn_accuracies = kfold_CV(MultinomialNB(), X, y, k=4)\n",
    "\n",
    "our_avg = np.mean(accuracies)\n",
    "sklearn_avg = np.mean(sklearn_accuracies)\n",
    "\n",
    "print(f\"sklearn kfold {sklearn_accuracies} vs my {accuracies}\")\n",
    "print(np.abs(sklearn_avg-true_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
